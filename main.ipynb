{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivPt9VEbreTj"
      },
      "source": [
        "This is the official python implementation for paper ***Neha Sharma, Chhavi Dhiman, S. Indu***, \"Visual-Motion-Interaction Guided Pedestrian Intention Prediction Framework\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvOHNViiEzQd",
        "outputId": "22851054-4eae-4fec-f9a2-b52308f2dbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#For importing dataset and helper codes to load and process it\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93rIrAo84aDC",
        "outputId": "db56fe58-b76a-403a-cec0-14f465b9a9a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1XNSLZP2RbjH7VCNXR4G2gvL7PZBv73oX/vmi\n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: random\n",
            "seq_type: crossing\n",
            "min_track_size: 75\n",
            "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
            "kfold_params: {'num_folds': 5, 'fold': 1}\n",
            "subset: default\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/MyDrive/vmi/PIE/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating crossing data\n",
            "Random sample currently exists.\n",
            " Loading from /content/drive/MyDrive/vmi/PIE/data_cache/random_samples.pkl\n",
            "The ratios are [0.5, 0.4, 0.1]\n",
            "Number of train tracks 890\n",
            "Subset: train\n",
            "Number of pedestrians: 890 \n",
            "Total number of samples: 807 \n",
            "---------------------------------------------------------\n",
            "Generating trajectory sequence data\n",
            "fstride: 1\n",
            "sample_type: all\n",
            "height_rng: [0, inf]\n",
            "squarify_ratio: 0\n",
            "data_split_type: random\n",
            "seq_type: crossing\n",
            "min_track_size: 75\n",
            "random_params: {'ratios': None, 'val_data': True, 'regen_data': False}\n",
            "kfold_params: {'num_folds': 5, 'fold': 1}\n",
            "subset: default\n",
            "---------------------------------------------------------\n",
            "Generating database for pie\n",
            "pie annotations loaded from /content/drive/MyDrive/vmi/PIE/data_cache/pie_database.pkl\n",
            "---------------------------------------------------------\n",
            "Generating crossing data\n",
            "Random sample currently exists.\n",
            " Loading from /content/drive/MyDrive/vmi/PIE/data_cache/random_samples.pkl\n",
            "The ratios are [0.5, 0.4, 0.1]\n",
            "Number of val tracks 178\n",
            "Subset: val\n",
            "Number of pedestrians: 178 \n",
            "Total number of samples: 156 \n",
            "\n",
            "#####################################\n",
            "Generating balanced raw data\n",
            "#####################################\n",
            "Unbalanced: \t Positive: 177 \t Negative: 630\n",
            "Balanced:\t Positive: 354  \t Negative: 354\n",
            "\n",
            "\n",
            "#####################################\n",
            "Getting poses train\n",
            "#####################################\n",
            "data/features/pie/poses/\n",
            "dict_keys(['set01', 'set02', 'set03', 'set04', 'set05', 'set06'])\n",
            "[####################] 99.86% \n",
            "#####################################\n",
            "Generating local box train\n",
            "#####################################\n",
            "Generating train features crop_type=bbox crop_mode=pad_resize              \n",
            "save_path=data/features/pie/local_box/, \n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
            "71686520/71686520 [==============================] - 0s 0us/step\n",
            "[--------------------] 0.00% "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1XNSLZP2RbjH7VCNXR4G2gvL7PZBv73oX/vmi/CBAM_SE.py:68: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  avg_pool = tf.compat.v1.layers.dense(inputs=avg_pool,\n",
            "/content/drive/.shortcut-targets-by-id/1XNSLZP2RbjH7VCNXR4G2gvL7PZBv73oX/vmi/CBAM_SE.py:76: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  avg_pool = tf.compat.v1.layers.dense(inputs=avg_pool,\n",
            "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer VarianceScaling is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
            "  warnings.warn(\n",
            "/content/drive/.shortcut-targets-by-id/1XNSLZP2RbjH7VCNXR4G2gvL7PZBv73oX/vmi/CBAM_SE.py:87: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  max_pool = tf.compat.v1.layers.dense(inputs=max_pool,\n",
            "/content/drive/.shortcut-targets-by-id/1XNSLZP2RbjH7VCNXR4G2gvL7PZBv73oX/vmi/CBAM_SE.py:94: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  max_pool = tf.compat.v1.layers.dense(inputs=max_pool,\n",
            "/content/drive/.shortcut-targets-by-id/1XNSLZP2RbjH7VCNXR4G2gvL7PZBv73oX/vmi/CBAM_SE.py:121: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
            "  concat = tf.compat.v1.layers.conv2d(concat,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[####################] 99.86% \n",
            "#####################################\n",
            "Generating local context train\n",
            "#####################################\n",
            "Generating train features crop_type=surround crop_mode=warp              \n",
            "save_path=data/features/pie/local_context/, \n",
            "[####################] 99.86% \n",
            "#####################################\n",
            "Generating balanced raw data\n",
            "#####################################\n",
            "Unbalanced: \t Positive: 177 \t Negative: 630\n",
            "Balanced:\t Positive: 354  \t Negative: 354\n",
            "\n",
            "\n",
            "#####################################\n",
            "Getting poses val\n",
            "#####################################\n",
            "data/features/pie/poses/\n",
            "dict_keys(['set01', 'set02', 'set03', 'set04', 'set05', 'set06'])\n",
            "[####################] 99.86% \n",
            "#####################################\n",
            "Generating local box val\n",
            "#####################################\n",
            "Generating val features crop_type=bbox crop_mode=pad_resize              \n",
            "save_path=data/features/pie/local_box/, \n",
            "[####################] 99.86% \n",
            "#####################################\n",
            "Generating local context val\n",
            "#####################################\n",
            "Generating val features crop_type=surround crop_mode=warp              \n",
            "save_path=data/features/pie/local_context/, \n",
            "[####################] 99.86% Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_pose (InputLayer)        [(None, 14, 36)]     0           []                               \n",
            "                                                                                                  \n",
            " input_box (InputLayer)         [(None, 14, 4)]      0           []                               \n",
            "                                                                                                  \n",
            " input_speed (InputLayer)       [(None, 14, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " input_local_box (InputLayer)   [(None, 14, 1792)]   0           []                               \n",
            "                                                                                                  \n",
            " input_local_context (InputLaye  [(None, 14, 1792)]  0           []                               \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " pose_conv (Conv1D)             (None, 12, 32)       3488        ['input_pose[0][0]']             \n",
            "                                                                                                  \n",
            " bb_conv (Conv1D)               (None, 12, 32)       416         ['input_box[0][0]']              \n",
            "                                                                                                  \n",
            " speed_conv (Conv1D)            (None, 12, 32)       128         ['input_speed[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 14, 3584)     0           ['input_local_box[0][0]',        \n",
            "                                                                  'input_local_context[0][0]']    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 12, 96)       0           ['pose_conv[0][0]',              \n",
            "                                                                  'bb_conv[0][0]',                \n",
            "                                                                  'speed_conv[0][0]']             \n",
            "                                                                                                  \n",
            " graph__convolution (Graph_Conv  (None, 14, 512)     3679246     ['input_local_context[0][0]']    \n",
            " olution)                                                                                         \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 14, 128)     1868288     ['concatenate_1[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 12, 128)      82432       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirectional  (None, 14, 128)     295424      ['graph__convolution[0][0]']     \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " attention_1 (attention)        (None, 128)          142         ['bidirectional_1[0][0]']        \n",
            "                                                                                                  \n",
            " attention (attention)          (None, 128)          140         ['bidirectional[0][0]']          \n",
            "                                                                                                  \n",
            " attention_2 (attention)        (None, 128)          142         ['bidirectional_2[0][0]']        \n",
            "                                                                                                  \n",
            " weighted_fusion (WeightedFusio  (None, 128)         5           ['attention_1[0][0]',            \n",
            " n)                                                               'attention[0][0]',              \n",
            "                                                                  'attention_2[0][0]']            \n",
            "                                                                                                  \n",
            " output_dense (Dense)           (None, 1)            129         ['weighted_fusion[0][0]']        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5,929,980\n",
            "Trainable params: 5,929,980\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/60\n",
            "45/45 [==============================] - 38s 271ms/step - loss: 0.9889 - accuracy: 0.4153 - val_loss: 0.9794 - val_accuracy: 0.4336\n",
            "Epoch 2/60\n",
            "45/45 [==============================] - 8s 180ms/step - loss: 0.9731 - accuracy: 0.4590 - val_loss: 0.9652 - val_accuracy: 0.4859\n",
            "Epoch 3/60\n",
            "45/45 [==============================] - 9s 198ms/step - loss: 0.9590 - accuracy: 0.5198 - val_loss: 0.9521 - val_accuracy: 0.5621\n",
            "Epoch 4/60\n",
            "45/45 [==============================] - 8s 185ms/step - loss: 0.9457 - accuracy: 0.5989 - val_loss: 0.9390 - val_accuracy: 0.6201\n",
            "Epoch 5/60\n",
            "45/45 [==============================] - 8s 182ms/step - loss: 0.9326 - accuracy: 0.6398 - val_loss: 0.9266 - val_accuracy: 0.6497\n",
            "Epoch 6/60\n",
            "45/45 [==============================] - 9s 194ms/step - loss: 0.9202 - accuracy: 0.6723 - val_loss: 0.9143 - val_accuracy: 0.6836\n",
            "Epoch 7/60\n",
            "45/45 [==============================] - 8s 178ms/step - loss: 0.9080 - accuracy: 0.6907 - val_loss: 0.9026 - val_accuracy: 0.7006\n",
            "Epoch 8/60\n",
            "45/45 [==============================] - 9s 192ms/step - loss: 0.8964 - accuracy: 0.7090 - val_loss: 0.8917 - val_accuracy: 0.7133\n",
            "Epoch 9/60\n",
            "45/45 [==============================] - 8s 180ms/step - loss: 0.8851 - accuracy: 0.7203 - val_loss: 0.8809 - val_accuracy: 0.7288\n",
            "Epoch 10/60\n",
            "45/45 [==============================] - 8s 187ms/step - loss: 0.8745 - accuracy: 0.7302 - val_loss: 0.8707 - val_accuracy: 0.7387\n",
            "Epoch 11/60\n",
            "45/45 [==============================] - 9s 189ms/step - loss: 0.8646 - accuracy: 0.7415 - val_loss: 0.8611 - val_accuracy: 0.7458\n",
            "Epoch 12/60\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 0.8545 - accuracy: 0.7486 - val_loss: 0.8518 - val_accuracy: 0.7571\n",
            "Epoch 13/60\n",
            "45/45 [==============================] - 9s 193ms/step - loss: 0.8452 - accuracy: 0.7528 - val_loss: 0.8429 - val_accuracy: 0.7571\n",
            "Epoch 14/60\n",
            "45/45 [==============================] - 8s 175ms/step - loss: 0.8364 - accuracy: 0.7627 - val_loss: 0.8343 - val_accuracy: 0.7571\n",
            "Epoch 15/60\n",
            "45/45 [==============================] - 9s 198ms/step - loss: 0.8278 - accuracy: 0.7627 - val_loss: 0.8261 - val_accuracy: 0.7585\n",
            "Epoch 16/60\n",
            "45/45 [==============================] - 9s 197ms/step - loss: 0.8198 - accuracy: 0.7684 - val_loss: 0.8185 - val_accuracy: 0.7655\n",
            "Epoch 17/60\n",
            "45/45 [==============================] - 8s 181ms/step - loss: 0.8121 - accuracy: 0.7698 - val_loss: 0.8109 - val_accuracy: 0.7627\n",
            "Epoch 18/60\n",
            "45/45 [==============================] - 9s 189ms/step - loss: 0.8043 - accuracy: 0.7754 - val_loss: 0.8039 - val_accuracy: 0.7669\n",
            "Epoch 19/60\n",
            "45/45 [==============================] - 8s 171ms/step - loss: 0.7970 - accuracy: 0.7782 - val_loss: 0.7969 - val_accuracy: 0.7740\n",
            "Epoch 20/60\n",
            "45/45 [==============================] - 9s 208ms/step - loss: 0.7900 - accuracy: 0.7782 - val_loss: 0.7902 - val_accuracy: 0.7768\n",
            "Epoch 21/60\n",
            "45/45 [==============================] - 9s 191ms/step - loss: 0.7837 - accuracy: 0.7797 - val_loss: 0.7838 - val_accuracy: 0.7768\n",
            "Epoch 22/60\n",
            "45/45 [==============================] - 8s 175ms/step - loss: 0.7769 - accuracy: 0.7839 - val_loss: 0.7775 - val_accuracy: 0.7782\n",
            "Epoch 23/60\n",
            "45/45 [==============================] - 8s 189ms/step - loss: 0.7706 - accuracy: 0.7867 - val_loss: 0.7713 - val_accuracy: 0.7811\n",
            "Epoch 24/60\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 0.7639 - accuracy: 0.7853 - val_loss: 0.7653 - val_accuracy: 0.7825\n",
            "Epoch 25/60\n",
            "45/45 [==============================] - 9s 193ms/step - loss: 0.7575 - accuracy: 0.7924 - val_loss: 0.7591 - val_accuracy: 0.7839\n",
            "Epoch 26/60\n",
            "45/45 [==============================] - 8s 182ms/step - loss: 0.7517 - accuracy: 0.7867 - val_loss: 0.7532 - val_accuracy: 0.7825\n",
            "Epoch 27/60\n",
            "45/45 [==============================] - 9s 205ms/step - loss: 0.7456 - accuracy: 0.7881 - val_loss: 0.7471 - val_accuracy: 0.7825\n",
            "Epoch 28/60\n",
            "45/45 [==============================] - 9s 193ms/step - loss: 0.7395 - accuracy: 0.7966 - val_loss: 0.7409 - val_accuracy: 0.7881\n",
            "Epoch 29/60\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 0.7333 - accuracy: 0.7980 - val_loss: 0.7349 - val_accuracy: 0.7867\n",
            "Epoch 30/60\n",
            "45/45 [==============================] - 9s 195ms/step - loss: 0.7262 - accuracy: 0.8037 - val_loss: 0.7288 - val_accuracy: 0.7910\n",
            "Epoch 31/60\n",
            "45/45 [==============================] - 8s 182ms/step - loss: 0.7194 - accuracy: 0.8051 - val_loss: 0.7222 - val_accuracy: 0.7994\n",
            "Epoch 32/60\n",
            "45/45 [==============================] - 9s 189ms/step - loss: 0.7139 - accuracy: 0.8107 - val_loss: 0.7158 - val_accuracy: 0.8051\n",
            "Epoch 33/60\n",
            "45/45 [==============================] - 9s 194ms/step - loss: 0.7066 - accuracy: 0.8136 - val_loss: 0.7089 - val_accuracy: 0.8079\n",
            "Epoch 34/60\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 0.6998 - accuracy: 0.8065 - val_loss: 0.7020 - val_accuracy: 0.8107\n",
            "Epoch 35/60\n",
            "45/45 [==============================] - 9s 199ms/step - loss: 0.6952 - accuracy: 0.8121 - val_loss: 0.6953 - val_accuracy: 0.8136\n",
            "Epoch 36/60\n",
            "45/45 [==============================] - 9s 198ms/step - loss: 0.6866 - accuracy: 0.8220 - val_loss: 0.6882 - val_accuracy: 0.8136\n",
            "Epoch 37/60\n",
            "45/45 [==============================] - 8s 186ms/step - loss: 0.6756 - accuracy: 0.8277 - val_loss: 0.6807 - val_accuracy: 0.8164\n",
            "Epoch 38/60\n",
            "45/45 [==============================] - 9s 198ms/step - loss: 0.6657 - accuracy: 0.8263 - val_loss: 0.6732 - val_accuracy: 0.8192\n",
            "Epoch 39/60\n",
            "45/45 [==============================] - 8s 175ms/step - loss: 0.6605 - accuracy: 0.8249 - val_loss: 0.6645 - val_accuracy: 0.8220\n",
            "Epoch 40/60\n",
            "45/45 [==============================] - 9s 196ms/step - loss: 0.6542 - accuracy: 0.8319 - val_loss: 0.6562 - val_accuracy: 0.8249\n",
            "Epoch 41/60\n",
            "45/45 [==============================] - 9s 190ms/step - loss: 0.6461 - accuracy: 0.8347 - val_loss: 0.6484 - val_accuracy: 0.8277\n",
            "Epoch 42/60\n",
            "45/45 [==============================] - 10s 219ms/step - loss: 0.6325 - accuracy: 0.8404 - val_loss: 0.6394 - val_accuracy: 0.8319\n",
            "Epoch 43/60\n",
            "45/45 [==============================] - 9s 194ms/step - loss: 0.6299 - accuracy: 0.8418 - val_loss: 0.6313 - val_accuracy: 0.8404\n",
            "Epoch 44/60\n",
            "45/45 [==============================] - 8s 182ms/step - loss: 0.6135 - accuracy: 0.8418 - val_loss: 0.6216 - val_accuracy: 0.8390\n",
            "Epoch 45/60\n",
            "45/45 [==============================] - 9s 197ms/step - loss: 0.6001 - accuracy: 0.8517 - val_loss: 0.6127 - val_accuracy: 0.8460\n",
            "Epoch 46/60\n",
            "45/45 [==============================] - 9s 191ms/step - loss: 0.5977 - accuracy: 0.8602 - val_loss: 0.6028 - val_accuracy: 0.8460\n",
            "Epoch 47/60\n",
            "45/45 [==============================] - 8s 183ms/step - loss: 0.5804 - accuracy: 0.8686 - val_loss: 0.5933 - val_accuracy: 0.8602\n",
            "Epoch 48/60\n",
            "45/45 [==============================] - 9s 193ms/step - loss: 0.5786 - accuracy: 0.8771 - val_loss: 0.5838 - val_accuracy: 0.8588\n",
            "Epoch 49/60\n",
            "45/45 [==============================] - 8s 173ms/step - loss: 0.5734 - accuracy: 0.8799 - val_loss: 0.5744 - val_accuracy: 0.8715\n",
            "Epoch 50/60\n",
            "45/45 [==============================] - 8s 189ms/step - loss: 0.5578 - accuracy: 0.8842 - val_loss: 0.5649 - val_accuracy: 0.8771\n",
            "Epoch 51/60\n",
            "45/45 [==============================] - 8s 177ms/step - loss: 0.5350 - accuracy: 0.9096 - val_loss: 0.5552 - val_accuracy: 0.8842\n",
            "Epoch 52/60\n",
            "45/45 [==============================] - 8s 188ms/step - loss: 0.5336 - accuracy: 0.9068 - val_loss: 0.5452 - val_accuracy: 0.8927\n",
            "Epoch 53/60\n",
            "45/45 [==============================] - 8s 183ms/step - loss: 0.5220 - accuracy: 0.9153 - val_loss: 0.5364 - val_accuracy: 0.9025\n",
            "Epoch 54/60\n",
            "45/45 [==============================] - 8s 179ms/step - loss: 0.5254 - accuracy: 0.9138 - val_loss: 0.5283 - val_accuracy: 0.9110\n",
            "Epoch 55/60\n",
            "45/45 [==============================] - 9s 191ms/step - loss: 0.5091 - accuracy: 0.9223 - val_loss: 0.5190 - val_accuracy: 0.9181\n",
            "Epoch 56/60\n",
            "45/45 [==============================] - 8s 176ms/step - loss: 0.4982 - accuracy: 0.9393 - val_loss: 0.5107 - val_accuracy: 0.9251\n",
            "Epoch 57/60\n",
            "45/45 [==============================] - 9s 191ms/step - loss: 0.4957 - accuracy: 0.9336 - val_loss: 0.5036 - val_accuracy: 0.9322\n",
            "Epoch 58/60\n",
            "45/45 [==============================] - 9s 196ms/step - loss: 0.4682 - accuracy: 0.9534 - val_loss: 0.4957 - val_accuracy: 0.9350\n",
            "Epoch 59/60\n",
            "45/45 [==============================] - 8s 184ms/step - loss: 0.4759 - accuracy: 0.9477 - val_loss: 0.4898 - val_accuracy: 0.9449\n",
            "Epoch 60/60\n",
            "45/45 [==============================] - 9s 193ms/step - loss: 0.4552 - accuracy: 0.9633 - val_loss: 0.4829 - val_accuracy: 0.9492\n",
            "Weights are [<tf.Tensor: shape=(), dtype=float32, numpy=0.9999999>, <tf.Tensor: shape=(), dtype=float32, numpy=-0.9999999>, <tf.Tensor: shape=(), dtype=float32, numpy=0.9999999>]\n",
            "Train model is saved to data/models/pie/pie/final/model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla, dense_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote configs to data/models/pie/pie/final/configs.txt\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/vmi'\n",
        "\n",
        "import sys\n",
        "from vmi import VMI\n",
        "\n",
        "#Importing dataset\n",
        "sys.path.insert(0,'/content/drive/MyDrive/vmi/PIE')\n",
        "from pie_data import PIE\n",
        "\n",
        "#Configuring Data options\n",
        "data_opts ={'fstride': 1,\n",
        "            'subset': 'default',\n",
        "            'data_split_type': 'random',  # kfold, random, default\n",
        "            'seq_type': 'crossing',\n",
        "            'min_track_size': 75} ## for obs length of 15 frames + 60 frames tte. This should be adjusted for different setup\n",
        "imdb = PIE(data_path='/content/drive/MyDrive/vmi/PIE') # change with the path to the dataset\n",
        "\n",
        "#Configuring model options\n",
        "model_opts = {'obs_input_type': ['local_box', 'local_context','pose', 'box', 'speed'],\n",
        "              'enlarge_ratio': 1.5,\n",
        "              'pred_target_type': ['crossing'],\n",
        "              'obs_length': 15,  # Determines min track size\n",
        "              'time_to_event': 60, # Determines min track size\n",
        "              'dataset': 'pie',\n",
        "              'normalize_boxes': True}\n",
        "\n",
        "method_class = VMI()\n",
        "\n",
        "#Preparing training and validation data\n",
        "beh_seq_train = imdb.generate_data_trajectory_sequence('train', **data_opts)\n",
        "beh_seq_val = imdb.generate_data_trajectory_sequence('val', **data_opts)\n",
        "\n",
        "#Training and saving the model file\n",
        "saved_files_path, trained_model = method_class.train(beh_seq_train, model_opts=model_opts, epochs=60, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISfnDclms86i"
      },
      "outputs": [],
      "source": [
        "#Testing the model\n",
        "\n",
        "saved_files_path='/content/drive/MyDrive/sf_gru/data/models/pie/pie/final'\n",
        "beh_seq_train = imdb.generate_data_trajectory_sequence('test', **data_opts)\n",
        "acc, auc, f1, precision, recall = method_class.test(beh_seq_train, saved_files_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WtCVatdRkQIl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}